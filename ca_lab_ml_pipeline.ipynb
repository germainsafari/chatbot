{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE: Introduction to Machine Learning Pipelines with scikit-learn\n",
    "## Author: Andrea Giussani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is to challenge you on building a supervised machine learning pipeline to predict the median values of owner-occupied housing in USD 1000, denoted as MEDV. We are going to famous Boston dataset, which contains a set of different features that are used to predict the MEDV target variable. Here, you will be guided with an hands-on exercise on data preprocessing, fitting and evaluation of a regression model. \n",
    "\n",
    "To get the most from this lab, it is recommended to have confidence and exposure to the following libraries: `pandas`, `matplotlib` and `scikit-learn`.\n",
    "\n",
    "I strongly encourage you to have watched the following courses, available in our content library:\n",
    " - Building a Machine Learning pipeline with scikit-learn: part 1 <br>\n",
    " - Building a Machine Learning pipeline with scikit-learn: part 2 <br>\n",
    "\n",
    "before starting this hands-on lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the Boston dataset using the pandas `pd.read_csv` method. We also inspect the first five rows by applying the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYh5BcsU3Vdg"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "boston_df = pd.read_csv('data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "8CCwjxxi4tCk",
    "outputId": "1008d11b-6240-4167-d9e8-6b3605bcb0ce"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston data frame has 506 observations and 13 features. Among many, it is worth mentioning the `INDUS` and the `NOX` columns, which describe `proportion of non-retail business acres per town` and  `nitrogen oxides concentration (parts per 10 million)`, respectively. To get a better understanding of what's going on, let us at first separate the target variable MEDV from the set of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jElIlvoK-Fb7"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X = boston_df.drop(['MEDV'], axis=1)\n",
    "y = boston_df['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the statistical distribution of each single feature, using the classical boxplot method. Remember: a Box Plot is the visual representation of five statistical summary statistics of a given feature. Those five statistical measures are: the minimum, the 25th, 50th, 75th percentile, and the maximum value.  Box plots therefore use robust summary statistics that can be quickly computed by hand, and have no tuning parameters. They are particularly useful for comparing distributions across groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "c43fPYXe4uGA",
    "outputId": "1658f67b-9ed7-4b96-9079-a3c70b0e797b"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "ax = sns.boxplot(x='variable', y='value', data=pd.melt(X))\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('MEDV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot a potential problem here? Well there are many. One is that different features are expressed in different order of magnitude. What does in mean practically? It means that, for instance, `TAX` is expressed in hundreds, whereas `INDUS` is a binary variable. Also, some variables are skewed ... It makes sense to scale the data, meaning to normalize each single data point by subtracting the mean and diving by the feature standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAhcifug52ye"
   },
   "source": [
    "## 2. Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we use the `StandardScaler` scikit-learn transformer. Remember: a transformer is characterized by two methods. A `fit`, which learn patterns in the data. And a `predict`, which uses those patterns to transform the original dataset. For the `StandardScaler`, the mean and the standard deviation are learnt during the fit phase, and then the new scaled dataset is obtained by transform ing the original one using those information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oc9D9smg5uWf"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f23fW1JZ-QMQ"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the new scaled data inside a `pandas.DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avFG23Fh_AMQ"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "scaled_df = pd.DataFrame(X_scaled, columns=list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# Validation Check\n",
    "# DO NOT CHANGE THIS CELL\n",
    "# ====================================\n",
    "assert type(scaled_df) == pd.DataFrame\n",
    "with open('results/vcf_ml_pipe_01.txt', 'w') as f:\n",
    "    for item in [round(scaled_df.iloc[0,0], 2)]:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the scaled features: as you can see, now we have remove the magnitude effect, but the skewness effect is still present in each single feature. That is not a big deal, but please note that there exists different methods that allow to further clean the data: one possibility is to fully force the data to be Gaussian: this method is typically used in time-series analysis (ARMA, ARIMA) so for this use case we can proceed with the plain-vanilla scaled data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "pUuNb21UAhbK",
    "outputId": "10736723-caa6-4461-c0fb-2db9901f9d6c"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "ax = sns.boxplot(x='variable', y='value', data=pd.melt(scaled_df))\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('MEDV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bITOojPAm3J"
   },
   "source": [
    "## 3. Fit a Ridge Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fit a Ridge regressor on the scaled data. In scikit-learn, a Ridge model belongs to the class of estimators. The Ridge model makes a trade-off between the simplicity of the model (near-zero coefficients) and its performance on the training set. A Ridge model is going to add a penalty term on the parameter estimates, which is controlled by the hyper-parameter $\\alpha$: we expect the coeffcient estimates to be much smaller, in terms of L2 norm, when a large value of α is used, as compared to when a small value of α is used. For more details, please check the course `Building Machine Learning Pipelines - Part 2` available in our content library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmEE-wekAkgX"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Ridge estimator is made of two main methods: a fit and a predict. A fit is applied on the training data, so that the model learns specific patterns from it. And a predict, which uses the trained patterns to make prediction on new, unseen data.\n",
    "\n",
    "Fitting an estimator in scikit-learn is pretty straightforward: we just need to initialize an estimator class - in our case a Ridge estimator - and then calling on it the fit method by passing the scaled features and the (training) target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-YEyovvCPX2",
    "outputId": "b9914ba6-4b5b-4d1f-9a94-f93b3abc8e80"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to predict the `MEDV` value for the test set. To do so, we need to apply the `predict()` method to the dfitted ridge model. However, note that we need to transform the data, since the model is expecting a specific data pattern as input. To accomplish this task, we just need to call the `transform()` method from the scaler: `scaler.transform(X_test)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mfcq_roECsxn",
    "outputId": "288f4d26-7b57-4881-8cc8-9fb6007314a7"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "y_pred = ridge.predict(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have got a Ridge model, as well as a prediction set, we can evaluate the model's performances: to do so, we can call the `score` method on the model, which by default, is going to compute the $R2$, the gold-standard for linear models. Let us compute the $R2$ for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsacaRXBGDSQ",
    "outputId": "ace14332-3873-489d-90e6-38410983bf7d"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "r2_test = ridge.score(scaler.transform(X_test), y_test)\n",
    "print(f'The R2 score on the test set is {r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the model is doing well, considering we fitted a simple Ridge model. However, there are two important remarks to make here:\n",
    "\n",
    " * on the one hand, this result might be given by chance, namely it might depend on the way we splitted the training data;\n",
    " * on the other hand, we have not taken into account any hyperparameter in the initialization of the model. However, we know that the ridge is heavily dependent on the choice of the hyperparameter $\\alpha$, which is by default set to 1.  \n",
    "\n",
    "Hence, it might be useful to optimize the fitting procedure taking into account a general solution that can help us in dealing with those two issues, namely by implementing the so-called Grid Search Cross-Validation (CV). To get more details on this estimator, please check the course `Building a Machine Learning pipeline with scikit-learn: part 2` in our content library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9DyXACAKyI4"
   },
   "source": [
    "## 4. Improving the model performances: GridSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into `GridSearchCV` class, let us focus on another scikit-learn feature: the pipeline object. With this class, you can simply  define all the steps your machine learning flow has to take into account in both training and test. If you remember, in our pipeline we have two important steps: the transformation one using the StandardScaler, and the fitting using a Ridge model. We can join together those two steps using the `Pipeline` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4EkIo0aIkP3"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "ml_pipe = Pipeline(\n",
    "    [\n",
    "     ('scaler', StandardScaler()),\n",
    "     ('model', Ridge())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit the pipeline object using a GridSearchCV estimator: this class requires you to specify the `model` - in our case the pipeline - and the `param_grid`. Since we want to find the best $\\alpha$ parameter, we can take into account the following param_grid:\n",
    "```python\n",
    "param_grid = {'model__alpha': np.logspace(-3,3,10)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltb1oyjRJxPf",
    "outputId": "d11203a7-1447-4b5e-a3dd-d70211f5d3af"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "cv = GridSearchCV(ml_pipe, param_grid = {'model__alpha': np.logspace(-3,3,10)}, cv=10)\n",
    "cv.fit(X_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1rqSPzgJ-vC",
    "outputId": "7e4d27f2-5250-4845-93df-cbc1f420ca42"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/v-gsafari/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "cv.score(scaler.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeHdK2_yF_yL"
   },
   "source": [
    "**End Lab**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Overview of ML Pipelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
